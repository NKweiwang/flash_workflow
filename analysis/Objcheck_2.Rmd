---
title: "objective function with the penalty term"
author: "wei"
date: YYYY-MM-DD
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

In this case, we would like to try the objective function with penalty term.

I would like to modify the flash code in this document.

```{r,echo=FALSE}
source("../code/flash_reg_170620.R")
```


```{r}
sim_K = function(K, N, P, SF, SL, signal,noise){
  E = matrix(rnorm(N*P,0,noise),nrow=N)
  Y = E
  L_true = array(0, dim = c(N,K))
  F_true = array(0, dim = c(P,K))
  
  for(k in 1:K){
    lstart = rnorm(N, 0, signal)
    fstart = rnorm(P, 0, signal)
    
    index = sample(seq(1:N),(N*SL))
    lstart[index] = 0
    index = sample(seq(1:P),(P*SF))
    fstart[index] = 0
    
    L_true[,k] = lstart
    F_true[,k] = fstart
    
    Y = Y + lstart %*% t(fstart)
  }
  return(list(Y = Y, L_true = L_true, F_true = F_true, Error = E))
}
```

### check the new obj function

we check that the new objective function with penalty term does monotonically increase.

```{r}
set.seed(99)
objcheck_1 = function(K=1,N = 20, P=30, SF = 0.5, SL = 0.6, signal = 1,noise = 1){
  par(mfrow = c(3, 2))
  par(cex = 0.6)
  par(mar = c(3, 3, 0.8, 0.8), oma = c(1, 1, 1, 1))
  data = sim_K(K,N, P, SF , SL , signal ,noise )
  Y = data$Y
  E = data$Error
  g1 = flash(Y,objtype = "l")
  plot(g1$obj_val_track,main = "fdr method with penalized obj",type = "l")
  g2 =  flash(Y,objtype = "l",ash_para = list(method = "shrink"))
  plot(g2$obj_val_track,main = "shrink method without penalized obj",type = "l")
  g3 =  flashr::flash(Y,objtype = "l",ash_para = list(method = "shrink"))
  plot(g3$obj_val_track,main = "shrink method without penalized obj(old)",type = "l")
  g4 =  flashr::flash(Y,objtype = "l")
  plot(g4$obj_val_track,main = "fdr method without penalized obj",type = "l")
  plot(c(g1$obj_val,g2$obj_val,g3$obj_val,g4$obj_val),main = "obj value")
  RMSE = c(sqrt(mean((Y - g1$l %*% t(g1$f) -E)^2)) ,
           sqrt(mean((Y - g2$l %*% t(g2$f) -E)^2)) ,
           sqrt(mean((Y - g3$l %*% t(g3$f) -E)^2)) ,
           sqrt(mean((Y - g4$l %*% t(g4$f) -E)^2)) )
  plot(RMSE,main = "RMSE")
}
```


```{r}
objcheck_1(K=1,N = 20, P=30, SF = 0.5, SL = 0.6, signal = 1,noise = 1)
```

```{r}
objcheck_1(K=1,N = 200, P=300, SF = 0.8, SL = 0.6, signal = 1,noise = 1)
```

```{r}
objcheck_1(K=1,N = 200, P=300, SF = 0.9, SL = 0.8, signal = 1,noise = 1)
```

```{r}
objcheck_1(K=1,N = 200, P=300, SF = 0.5, SL = 0.9, signal = 1,noise = 3)
```

```{r}
objcheck_1(K=1,N = 100, P=200, SF = 0.8, SL = 0.9, signal = 1,noise = 3)
```

we can see that the old version of code without penalty doesn't have monotonically increasing objective function, but the new version code with penalty term does.

*now we have two methods (fdr, shrink), and both of them have monotonically increasing corresponding objective function in each case.*

## try the same problem: rank 0 vs rank 1 with  fdr method.

```{r,echo = FALSE}

rank0check = function(K=1,N = 100, P=200, SF = 0.5, SL = 0.5, signal = 0,noise = 1, mtype = "fdr"){
  data = sim_K(K,N, P, SF , SL , signal ,noise )
  Y = data$Y
  E = data$Error
  if(mtype == "fdr"){
    g1 = flash(Y,objtype = "l")
  }else{
    g1 = flash(Y,objtype = "l",ash_para = list(method = "shrink"))
  }
  
  RMSE = sqrt(mean((Y - g1$l %*% t(g1$f) -E)^2)) 
  RMSE0 = sqrt(mean((Y  -E)^2)) 
  obj_val = g1$obj_val
  # to get the rank zero objective value
  sigmae2_v0 = Y^2
  fit_g0 = list(pi=c(1,0,0),sd = c(0,0.1,1))
  mat0 = list(comp_postmean = matrix(0,ncol= N,nrow = 3),
              comp_postmean2 = matrix(0,ncol= N,nrow = 3),
              comp_postprob = matrix(0,ncol= N,nrow = 3))
  mat0$comp_postprob[1,] = 1
  par_l0 = list(g = fit_g0, mat = mat0)
  fit_g0 = list(pi=c(1,0,0),sd = c(0,0.1,1))
  mat0 = list(comp_postmean = matrix(0,ncol= P,nrow = 3),
              comp_postmean2 = matrix(0,ncol= P,nrow = 3),
              comp_postprob = matrix(0,ncol= P,nrow = 3))
  mat0$comp_postprob[1,] = 1
  par_f0 = list(g = fit_g0, mat = mat0)
  obj0 = obj(N,P,sigmae2_v0,sigmae2 = mean(sigmae2_v0),par_f0,par_l0,objtype = "lowerbound_lik",ash_para = list(method = NULL))
  
  return(list(rmse = RMSE,rmse0 = RMSE0,obj_val = obj_val,obj_0 = obj0))
}

sim_rank0 = function(K=1,N = 100, P=200, SF = 0.5, SL = 0.5, signal = 0,noise = 1,mtype = "fdr"){
  T = 200
  RMSE = rep(NA,T)
  diff_obj = rep(NA,T)
  for(t in 1:T){
    g0 = rank0check(K = 1,N, P, SF, SL , signal,noise,mtype = mtype)
    RMSE[t] = g0$rmse
    diff_obj[t] = (g0$obj_val - g0$obj_0)
  }
  par(mfrow = c(2, 2))
  par(cex = 0.6)
  par(mar = c(3, 3, 0.8, 0.8), oma = c(1, 1, 1, 1))
  boxplot(RMSE,main = "RMSE")
  hist(diff_obj, breaks = 50,main = "sign of difference of obj value")
  hist(RMSE,breaks = 50,main = "RMSE")
  plot(diff_obj,RMSE, main = "sign of obj diff vs RMSE")
  return(c(sum(diff_obj == 0),sum(diff_obj>0),sum(diff_obj<0))/T)
}

```


#### case 1: rank 0

we compare the old code with our new code, we can see that most of the cases a

*shrink method without penalty*

```{r}
set.seed(99)
sim_rank0(K=1,N = 100, P=200, SF = 0.5, SL = 0.5, signal = 0,noise = 1,mtype = "shrink")
```

*fdr method with penalty*

```{r}
set.seed(99)
sim_rank0(K=1,N = 100, P=200, SF = 0.5, SL = 0.5, signal = 0,noise = 1)
```

we can see the improvement that the proportion of P(estimated structure better than rank 0 structure) is much larger.

#### case 2: rank 1, very sparse with large noise

*shrink method without penalty*

```{r}
set.seed(99)
sim_rank0(K=1,N = 100, P=200, SF = 0.8, SL = 0.8, signal = 1,noise = 2,mtype = "shrink")
```

*fdr method with penalty*

```{r}
set.seed(99)
sim_rank0(K=1,N = 100, P=200, SF = 0.8, SL = 0.8, signal = 1,noise = 2)
```

we can see that the results are improved since the proportion of P(estimated structure better than rank 0 structure) is much larger.

In this case we can see that the RMSE is negative correlated with the objective function value. So it is make sense that we return $g_l = 0, g_f = 0$ if the $LB(estimated) < LB(g_l=0,g_f = 0)$.

## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
