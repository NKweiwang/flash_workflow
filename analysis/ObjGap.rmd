---
title: "'gap' issue"
author: "Wei Wang"
date: 
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

## introduction of the problem.

In the old code of flash. we found that the greedy algorithm doesn't increase the lower bound of the log-likelihood. So we turn to understand the likelihood of rank-one and rank-zero model.

Here I will rerun all the representative code, and just provide the original document describing this issue.

### old version documnt

The old version document is in `ash-sfa` git repo [ash-sfa repo](https://github.com/stephenslab/ash-sfa).

In the document, we have several observations:

1. In the rank one FLASH, we can see that our algorithm monotonically increase that lower-bound of the log-likelihood. [show case in old code](https://github.com/stephenslab/ash-sfa/blob/master/Rcode/postmean/flash/simulation/convergereport.pdf)
2. In the greedy algorithm, the objective function is not increase in each time adding one more factor. We thought the penalty term in `ash` might cause this problem, and we find one case that the penalty term does have effect in rank one model. After we change `method = "fdr"` to `method = "shrink"` in `ash`, the objective function monotonically increase in the algorithm. But when we change the `mehtod ="shrink"` in `greedy` and `backfitting` function, the log-likelihood still doesn't increase in each time by adding a new factor. [show case in old code](https://github.com/stephenslab/ash-sfa/blob/master/Rcode/postmean/flash/simulation/testKfactorVEM.pdf).
3. Then we know that change `method = shrink` can guarantee that rank one and backfitting(if not add or remove factor) algorithm monotonic increase. The only problem is adding or removing factor, which cause the log-likelihood not monotonic. Then we turn to the simplest case of adding or removing factor: rank-one vs rank-zero model. In this document, one thing we can't understand well is that the prediction result seems not corresponding to the objective value. [show case in old code](https://github.com/stephenslab/ash-sfa/blob/master/Rcode/postmean/flash/simulation/objcheck.pdf)

We will rerun the analysis in this document. The above observations just show the old documents. 

The following is the main part of this document.

## examples

We will start from the problem in greedy algorithm:

```{r}
sim_K = function(K, N, P, SF, SL, signal,noise){
  E = matrix(rnorm(N*P,0,noise),nrow=N)
  Y = E
  L_true = array(0, dim = c(N,K))
  F_true = array(0, dim = c(P,K))
  
  for(k in 1:K){
    lstart = rnorm(N, 0, signal)
    fstart = rnorm(P, 0, signal)
    
    index = sample(seq(1:N),(N*SL))
    lstart[index] = 0
    index = sample(seq(1:P),(P*SF))
    fstart[index] = 0
    
    L_true[,k] = lstart
    F_true[,k] = fstart
    
    Y = Y + lstart %*% t(fstart)
  }
  return(list(Y = Y, L_true = L_true, F_true = F_true, Error = E))
}
```



## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
